# Logistic Regression - Vectorized Implementation | Regress√£o Log√≠stica - Implementa√ß√£o Vetorizada

> **üá∫üá∏ English version below** | **üáßüá∑ Vers√£o em portugu√™s abaixo**

---

## üá∫üá∏ English

A from-scratch implementation of Logistic Regression using NumPy for binary classification on the Framingham Heart Study dataset.

### Overview

This notebook implements the core logistic regression algorithm manually, focusing on understanding the mathematical foundations of the algorithm rather than using pre-built ML libraries.

### Dataset

**Framingham Heart Study** - Cardiovascular disease prediction dataset
- 4,238 samples with 15 features (demographics, risk factors, health metrics)
- Binary classification: 10-year CHD risk prediction
- Train/Test Split: 80/20 (3,390 / 848 samples)

### Implementation Details

#### ‚úÖ Implemented From Scratch

- **Sigmoid function**: Logistic activation function
- **Cost function**: Binary cross-entropy (log loss)
- **Gradient computation**: Fully vectorized gradient calculation
- **Gradient descent**: Iterative optimization algorithm
- **Prediction function**: Binary classification with configurable threshold

#### ‚ö†Ô∏è External Dependencies (Non-Algorithm)

The following sklearn utilities are used for **data preprocessing only**:
- `train_test_split`: Dataset splitting
- `SimpleImputer`: Missing value imputation
- `StandardScaler`: Feature normalization
- `confusion_matrix`, `classification_report`: Evaluation metrics

**Note**: These are infrastructure utilities and do not affect the core algorithm implementation, which is 100% from scratch.

### Mathematical Foundation

#### Sigmoid Function
```
œÉ(z) = 1 / (1 + e^(-z))
```

#### Cost Function (Binary Cross-Entropy)
```
J(w,b) = -1/m * Œ£[y*log(h(x)) + (1-y)*log(1-h(x))]
```

#### Gradient Descent Update
```
w = w - Œ± * ‚àÇJ/‚àÇw
b = b - Œ± * ‚àÇJ/‚àÇb
```

Where:
- `‚àÇJ/‚àÇw = 1/m * X^T * (h(X) - y)` (vectorized)
- `‚àÇJ/‚àÇb = 1/m * Œ£(h(x) - y)`

### Results

#### Model Performance

- **Test Accuracy**: 63.68%
- **Precision (Class 1)**: 0.24
- **Recall (Class 1)**: 0.66
- **F1-Score (Class 1)**: 0.35

#### Analysis

The model shows high recall but low precision for the positive class (CHD risk). This is expected given:
1. Highly imbalanced dataset (~15% positive cases)
2. Medical prediction task where missing true positives is costly

The relatively low accuracy reflects the challenge of predicting cardiovascular risk from limited features and class imbalance.

### Key Features

- **Fully vectorized operations** using NumPy for computational efficiency
- **Epsilon smoothing** (1e-5) to prevent log(0) errors
- **Modular design** with separate functions for cost, gradient, and prediction
- **Training history tracking** for convergence analysis

### Usage

```python
# Initialize parameters
initial_w = np.zeros(n_features)
initial_b = 0.0

# Train model
w, b, J_history, w_history = gradient_descent(
    X_train_scaled,
    y_train,
    initial_w,
    initial_b,
    compute_cost,
    compute_gradient,
    alpha=0.001,        # learning rate
    num_iters=1000,     # iterations
    lambda_=0           # regularization
)

# Make predictions
predictions = predict(X_test_scaled, w, b, threshold=0.15)

# Evaluate
accuracy = np.mean(predictions == y_test) * 100
```

### Requirements

```bash
pip install numpy pandas matplotlib seaborn scikit-learn
```

### Educational Purpose

This implementation prioritizes **clarity and understanding** over production-ready performance. It demonstrates:
- How logistic regression works mathematically
- The power of vectorization in machine learning
- Gradient descent optimization from first principles
- Binary classification fundamentals

---

## üáßüá∑ Portugu√™s

Uma implementa√ß√£o do zero de Regress√£o Log√≠stica usando NumPy para classifica√ß√£o bin√°ria no dataset Framingham Heart Study.

### Vis√£o Geral

Este notebook implementa o algoritmo central de regress√£o log√≠stica manualmente, focando na compreens√£o dos fundamentos matem√°ticos do algoritmo ao inv√©s de usar bibliotecas de ML prontas.

### Dataset

**Framingham Heart Study** - Dataset de predi√ß√£o de doen√ßas cardiovasculares
- 4.238 amostras com 15 features (demografia, fatores de risco, m√©tricas de sa√∫de)
- Classifica√ß√£o bin√°ria: predi√ß√£o de risco cardiovascular em 10 anos
- Divis√£o Treino/Teste: 80/20 (3.390 / 848 amostras)

### Detalhes da Implementa√ß√£o

#### ‚úÖ Implementado do Zero

- **Fun√ß√£o sigmoid**: Fun√ß√£o de ativa√ß√£o log√≠stica
- **Fun√ß√£o de custo**: Entropia cruzada bin√°ria (log loss)
- **C√°lculo do gradiente**: C√°lculo vetorizado completo do gradiente
- **Gradiente descendente**: Algoritmo de otimiza√ß√£o iterativa
- **Fun√ß√£o de predi√ß√£o**: Classifica√ß√£o bin√°ria com threshold configur√°vel

#### ‚ö†Ô∏è Depend√™ncias Externas (N√£o-Algor√≠tmicas)

As seguintes utilidades do sklearn s√£o usadas **apenas para pr√©-processamento de dados**:
- `train_test_split`: Divis√£o do dataset
- `SimpleImputer`: Imputa√ß√£o de valores faltantes
- `StandardScaler`: Normaliza√ß√£o de features
- `confusion_matrix`, `classification_report`: M√©tricas de avalia√ß√£o

**Nota**: Estas s√£o utilidades de infraestrutura e n√£o afetam a implementa√ß√£o central do algoritmo, que √© 100% do zero.

### Fundamentos Matem√°ticos

#### Fun√ß√£o Sigmoid
```
œÉ(z) = 1 / (1 + e^(-z))
```

#### Fun√ß√£o de Custo (Entropia Cruzada Bin√°ria)
```
J(w,b) = -1/m * Œ£[y*log(h(x)) + (1-y)*log(1-h(x))]
```

#### Atualiza√ß√£o do Gradiente Descendente
```
w = w - Œ± * ‚àÇJ/‚àÇw
b = b - Œ± * ‚àÇJ/‚àÇb
```

Onde:
- `‚àÇJ/‚àÇw = 1/m * X^T * (h(X) - y)` (vetorizado)
- `‚àÇJ/‚àÇb = 1/m * Œ£(h(x) - y)`

### Resultados

#### Performance do Modelo

- **Acur√°cia no Teste**: 63.68%
- **Precision (Classe 1)**: 0.24
- **Recall (Classe 1)**: 0.66
- **F1-Score (Classe 1)**: 0.35

#### An√°lise

O modelo mostra alto recall mas baixa precis√£o para a classe positiva (risco cardiovascular). Isso √© esperado dado:
1. Dataset altamente desbalanceado (~15% casos positivos)
2. Tarefa de predi√ß√£o m√©dica onde perder positivos verdadeiros √© custoso

A acur√°cia relativamente baixa reflete o desafio de prever risco cardiovascular com features limitadas e desbalanceamento de classes.

### Caracter√≠sticas Principais

- **Opera√ß√µes totalmente vetorizadas** usando NumPy para efici√™ncia computacional
- **Suaviza√ß√£o epsilon** (1e-5) para prevenir erros de log(0)
- **Design modular** com fun√ß√µes separadas para custo, gradiente e predi√ß√£o
- **Rastreamento do hist√≥rico de treinamento** para an√°lise de converg√™ncia

### Uso

```python
# Inicializar par√¢metros
initial_w = np.zeros(n_features)
initial_b = 0.0

# Treinar modelo
w, b, J_history, w_history = gradient_descent(
    X_train_scaled,
    y_train,
    initial_w,
    initial_b,
    compute_cost,
    compute_gradient,
    alpha=0.001,        # taxa de aprendizado
    num_iters=1000,     # itera√ß√µes
    lambda_=0           # regulariza√ß√£o
)

# Fazer predi√ß√µes
predictions = predict(X_test_scaled, w, b, threshold=0.15)

# Avaliar
accuracy = np.mean(predictions == y_test) * 100
```

### Requisitos

```bash
pip install numpy pandas matplotlib seaborn scikit-learn
```

### Prop√≥sito Educacional

Esta implementa√ß√£o prioriza **clareza e compreens√£o** ao inv√©s de performance pronta para produ√ß√£o. Ela demonstra:
- Como a regress√£o log√≠stica funciona matematicamente
- O poder da vetoriza√ß√£o em machine learning
- Otimiza√ß√£o via gradiente descendente desde os primeiros princ√≠pios
- Fundamentos de classifica√ß√£o bin√°ria

---

## License | Licen√ßa

Educational project - Free to use and modify.

Projeto educacional - Livre para usar e modificar.
